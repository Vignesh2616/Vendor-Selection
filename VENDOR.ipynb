{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e1ee7cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (2407452009.py, line 115)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 115\u001b[1;36m\u001b[0m\n\u001b[1;33m    \\df_collect = df_collect.withColumn(\"Same_PIN\", when(col(\"HGIN_Supplier_Despatch_Source_Town_Pincode\") == col(\"MAB_Zip_Code\"), \"yes\").otherwise(\"no\"))\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import log\n",
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark=SparkSession.builder.appName('vendor_model').getOrCreate()\n",
    "\n",
    "dataframe = spark.read.csv(\"/EDWSTG40/PRC.ORD_H_Purchase_Orders\" , inferSchema = True , header = True)\n",
    "dataframe.show(5)\n",
    "df = dataframe.select('HPO_PO_Number','HPO_Delivery_Start_Date','HPO_Delivery_End_Date','HPO_BA_Code','HPO_PO_Date')\n",
    "\n",
    "\n",
    "dataframe1 = spark.read.csv(\"/EDWSTG40/PRC.ORD_D_Purchase_Orders\" , inferSchema = True , header = True)\n",
    "#dataframe1.show(5)\n",
    "df1 = dataframe1.select('DPO_Material_Code','DPO_PO_Number')\n",
    "\n",
    "df2 = df1.join(df, df1.DPO_PO_Number == df.HPO_PO_Number,\"left\")\n",
    "df2.show(5)\n",
    "\n",
    "\n",
    "\n",
    "# dataframe2 = spark.read.csv(\"/EDWSTG40/MDM.GEM_M_Materials\" , inferSchema = True , header = True)\n",
    "# df10 = dataframe2.select('HPO_PO_Number','HPO_PO_Basic_Value','HPO_PO_Net_Value','HPO_Delivery_Start_Date','HPO_Delivery_End_Date','HPO_BA_Code','HPO_PO_Date')\n",
    "\n",
    "# dataframe2.show(5)\n",
    "dataframe3 = spark.read.csv(\"/EDWSTG40/MDM.BAM_M_Business_Associates\" , inferSchema = True , header = True)\n",
    "df10 = dataframe3.select('MBA_Is_Blacklisted','MBA_BA_Code')\n",
    "df10.show(5)\n",
    "\n",
    "dataframe4 = spark.read.csv(\"/EDWSTG40/WHS.WRH_H_GIN\" , inferSchema = True , header = True)\n",
    "df11 = dataframe4.select('HGIN_PO_Number','HGIN_GIN_Date','HGIN_Supplier_Material_Despatch_Source_City_Code','HGIN_Supplier_Despatch_Source_Town_Pincode','HGIN_Supplier_Material_Despatch_Source_City_Code','HGIN_Warehouse_Code')\n",
    "df11.show(5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "df12 = df2.join(df11, df2.DPO_PO_Number == df11.HGIN_PO_Number,\"left\")\n",
    "df12.show(5)\n",
    "\n",
    "df13 = df12.join(df10,df12.HPO_BA_Code == df10.MBA_BA_Code,\"left\")\n",
    "df13.show(5)\n",
    "\n",
    "dataframe5 = spark.read.csv(\"/EDWSTG40/MDM.GEM_M_Warehouses\" , inferSchema = True , header = True)\n",
    "df14 = dataframe5.select('MWH_Warehouse_Code','MWH_AB_Code')\n",
    "df14.show(5)\n",
    "\n",
    "df20 = df13.join(df14, df13.HGIN_Warehouse_Code == df14.MWH_Warehouse_Code,\"left\")\n",
    "df20.show(5)\n",
    "\n",
    "\n",
    "\n",
    "dataframe6 = spark.read.csv(\"/EDWSTG40/MDM.GEM_M_Address_Book\" , inferSchema = True , header = True)\n",
    "df15 = dataframe6.select('MAB_AB_Code','MAB_Zip_Code','MAB_City_Code','MAB_State_Code')\n",
    "df15.show(5)\n",
    "\n",
    "df21 = df20.join(df15, df20.MWH_AB_Code == df15.MAB_AB_Code,\"left\")\n",
    "df21.show(5)\n",
    "\n",
    "dataframe7 = spark.read.csv(\"/BFIX1/EDWSTG40/DEN.EDW_MPP_PO_Denormalized\" , inferSchema = True , header = True)\n",
    "\n",
    "df30 = dataframe7.select('PO_No','Actual_Payment_Made_to_Vendor','PO_Amount')\n",
    "df30.show(5)\n",
    "\n",
    "\n",
    "df_p = df21.join(df30, df21.HPO_PO_Number == df30.PO_No,\"left\")\n",
    "df_p.show(5)\n",
    "\n",
    "\n",
    "df21 = df_p.drop('HPO_PO_Number')\n",
    "df21 = df21.drop('HGIN_PO_Number')\n",
    "df21 = df21.drop('MBA_BA_Code')\n",
    "df21.show(5)\n",
    "\n",
    "\n",
    "windowSpec=Window.partitionBy('DPO_Material_Code','HPO_BA_Code').orderBy('DPO_PO_Number')\n",
    "df4 = df21.withColumn(\"row_number\",row_number().over(windowSpec))\n",
    "# df4.show(5)\n",
    "\n",
    "df4.show(1)\n",
    "\n",
    "df6 = df4.select(\"*\").groupby('DPO_Material_Code','HPO_BA_Code').agg(max('row_number').alias(\"total_time\"))\n",
    "print(\"df6\")\n",
    "df6.show(5)\n",
    "\n",
    "# # df6.createOrReplaceTempView(\"df6\")\n",
    "\n",
    "# # df_6=spark.sql(\"Select DPO_Material_Code as Material_Code , HPO_BA_Code as BA_Code , total_time from df6\")\n",
    "# df_6 = df6.withColumn(\"Material_Code\",DPO_Material_Code)\n",
    "# df_61 = df_6.withColumn(\"BA_Code\",HPO_BA_Code)\n",
    "# # print(\"df_6\n",
    "# df_61.show(5)\n",
    "\n",
    "df61 = df6.withColumnRenamed(\"DPO_Material_Code\",\"Material_Code\")\n",
    "df62 = df61.withColumnRenamed(\"HPO_BA_Code\",\"BA_Code\")\n",
    "df62.show(5)\n",
    "\n",
    "df_collect=df4.join(df62,df4.HPO_BA_Code==df62.BA_Code,\"left\")\n",
    "\n",
    "df_collect.show(30)\n",
    "# df_collect = df_collect.drop('DPO_Material_Code')\n",
    "# df_collect = df_collect.drop('HPO_BA_Code')\n",
    "# df_collect.show(15)\n",
    "\n",
    "df_collect = df_collect.filter(df_collect.MBA_Is_Blacklisted != 'Y')\n",
    "\n",
    "\n",
    "df_collect = df_collect.withColumn(\"Actual_Delivery\", when((col(\"HGIN_GIN_Date\") >= col(\"HPO_Delivery_Start_Date\")) & (col(\"HGIN_GIN_Date\") <= col(\"HPO_Delivery_End_Date\")), \"yes\").otherwise(\"no\"))\n",
    "df_collect.show(5)\n",
    "\n",
    "df_collect = df_collect.withColumn(\"Same_PIN\", when(col(\"HGIN_Supplier_Despatch_Source_Town_Pincode\") == col(\"MAB_Zip_Code\"), \"yes\").otherwise(\"no\"))\n",
    "df_collect = df_collect.withColumn(\"Same_City\", when(col(\"HGIN_Supplier_Despatch_Source_City_Pincode\") == col(\"MAB_City_Code\"), \"yes\").otherwise(\"no\"))\n",
    "df_collect = df_collect.withColumn(\"Same_State\", when(col(\"HGIN_Supplier_Despatch_Source_State_Pincode\") == col(\"MAB_State_Code\"), \"yes\").otherwise(\"no\"))\n",
    "df_collect.show(5)\n",
    "\n",
    "\n",
    "df_collect = df_collect.withColumn(\"Price_Diff\", col(\"PO_Amount\") - col(\"Actual_Payment_Made_to_Vendor\"))\n",
    "df_collect.show(5)\n",
    "\n",
    "df_final = df_collect.select('Actual_Delivery','Same_PIN','Same_City','Same_State','total_time_vendor_is_choosen','price_diff','BA_code')\n",
    "#df_final.show(5)\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "\n",
    "label_indexer1 = StringIndexer(inputCol=\"Actual_Delivery\", outputCol=\"Delay\")\n",
    "df_final = label_indexer1.fit(df_final).transform(df_final)\n",
    "\n",
    "label_indexer2 = StringIndexer(inputCol=\"Same_PIN\", outputCol=\"Same_PINs\")\n",
    "df_final = label_indexer2.fit(df_final).transform(df_final)\n",
    "\n",
    "label_indexer3 = StringIndexer(inputCol=\"Same_City\", outputCol=\"Same_Citys\")\n",
    "df_final = label_indexer3.fit(df_final).transform(df_final)\n",
    "\n",
    "label_indexer4 = StringIndexer(inputCol=\"Same_State\", outputCol=\"Same_States\")\n",
    "df_final  = label_indexer4.fit(df_final).transform(df_final)\n",
    "\n",
    "label_indexer5 = StringIndexer(inputCol=\"BA_code\", outputCol=\"vendor\")\n",
    "df_final  = label_indexer5.fit(df_final).transform(df_final)\n",
    "\n",
    "df_final.show(5)\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df_reg = df_final.drop('BA_Code')\n",
    "assembler = VectorAssembler(inputCols = df_reg.columns, outputCol = 'features')\n",
    "output=assembler.transform(df_final)\n",
    "\n",
    "df_ml = output.select(['features' , 'BA_Code'])\n",
    "df_ml.show(5)\n",
    "\n",
    "train_data, test_data = output.randomSplit([0.8, 0.2])\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"BA_Code\", featuresCol=\"features\")\n",
    "\n",
    "model = rf.fit(train_data)\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "predictions.show(5)\n",
    "\n",
    "-- from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "-- evaluator = MulticlassClassificationEvaluator(labelCol=\"BA_Code\", predictionCol=\"prediction\")\n",
    "-- accuracy = evaluator.evaluate(predictions)\n",
    "-- print(\"Accuracy = %s\" % (accuracy))\n",
    "-- print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad45801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
